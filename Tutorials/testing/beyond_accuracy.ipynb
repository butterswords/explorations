{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Beyond Accuracy\n",
    "## Overview\n",
    "In a [paper](https://aclanthology.org/2020.acl-main.442/) from 2020, Ribeiro et al. proposed several types of tests that allow developers and evaluators to go beyond the metrics offered by most benchmarks and testing packages. In the last few years many companies have attempted to offer products along these lines as the need for more robust testing has become apparent.\n",
    "\n",
    "Since the explosion of LLMs into the mainstream many people have asked the critical question: ***how do I test to make sure this model or application will work as I intend it to?*** \n",
    "\n",
    "There have been many attempts to answer this question, with few of them providing much satisfaction. This tutorial attempts to give you some foundational knowledge to build your own approach. It relies on two critical test strategies from the paper:\n",
    ">1. An **Invariance test** (INV) is when we apply label-preserving perturbations to inputs and expect the model prediction to remain the same.\n",
    ">2. A **Directional Expectation test** (DIR) is similar, except that the label is expected to change in a certain way.\n",
    "\n",
    "You will note that these are both designed for predictive models, so we will have to extend them to LLMs as follows:\n",
    "1. An **Invariance test for an LLM** (INV_LLM) is when we apply *intention*-preserving perturbations to inputs and *expect the generation to remain aligned to the intention of the inputs*. (Example: if we invert the gender or race of a name we expect the generation to treat them equally if race or gender are not essential to the intent.)\n",
    "2. A **Directional expectation test for an LLM** (DIR_LLM) is similar, except *we perturb the inputs to alter its intent and expect the generation to alter according to our expectations.* (Example: if we want to test a model's ability to refuse a request we might add derogatory content to an input then monitor the number of times the model said something like \"I'm sorry, I can't help with that\".)\n",
    "\n",
    "NOTE: Both of these can be considered forms of adversarial testing and should be treated with respect as they may create responses from models, intentionally or unintentionally, capable of harming the people who access them.\n",
    "\n",
    "## Setting up a dead simple foundation\n",
    "We will start by setting up a simple foundation that allows us to identify a specific placeholder and replace it with real values to create a list of test cases. This function can work for both tests.\n",
    "\n",
    "NOTE: a prerequisite for this work is that you have to be able to call a model. There are two straightforward options:\n",
    "1. use a Hugging Face account as we'll use the hugginface hub to run against models\n",
    "2. run the models locally on your machine using ollama\n",
    "\n",
    "Please follow the instructions in the top level `readme.md` file for the option you wish to use before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import pandas as pandas\n",
    "import re\n",
    "import ollama"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
